{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration Examples: YOLOv8 Object Detection with Popular Frameworks\n",
    "\n",
    "This notebook demonstrates how to integrate the YOLOv8 object detection functionality with popular deep learning frameworks and applications. We'll show examples of working with:\n",
    "\n",
    "1. Hugging Face Spaces for web demo deployment\n",
    "2. Integration with PyTorch Lightning for structured training\n",
    "3. Working with different image formats and sources\n",
    "4. Integration with computer vision pipelines\n",
    "\n",
    "All examples ensure full compatibility with both Kaggle and Colab environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "First, let's set up the environment with all the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository if running in Colab/Kaggle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab or Kaggle\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'kaggle_secrets' in sys.modules\n",
    "\n",
    "# If we're in Colab or Kaggle, set up the environment\n",
    "if IN_COLAB or IN_KAGGLE:\n",
    "    # Clone the repository\n",
    "    !git clone -q https://github.com/yourusername/object-detection-yolo.git\n",
    "    %cd object-detection-yolo\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q ultralytics opencv-python ipywidgets matplotlib Pillow\n",
    "    !pip install -q gradio pytorch-lightning albumentations\n",
    "    \n",
    "    # Add the repository root to the Python path\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "    \n",
    "    print(f\"Setting up in {'Google Colab' if IN_COLAB else 'Kaggle'}\")\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Install additional dependencies if needed\n",
    "    !pip install -q gradio pytorch-lightning albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import common libraries\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "# Import notebook utilities\n",
    "try:\n",
    "    from utils.notebook_utils import setup_env, download_sample_images\n",
    "except ImportError:\n",
    "    # Define minimal versions if not available\n",
    "    def setup_env():\n",
    "        print(\"Environment setup simplified due to import error\")\n",
    "    \n",
    "    def download_sample_images(output_dir=\"sample_images\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        samples = [\n",
    "            {'url': 'https://ultralytics.com/images/zidane.jpg', 'name': 'person.jpg'},\n",
    "            {'url': 'https://ultralytics.com/images/bus.jpg', 'name': 'bus.jpg'}\n",
    "        ]\n",
    "        paths = []\n",
    "        for sample in samples:\n",
    "            path = os.path.join(output_dir, sample['name'])\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Downloading {sample['name']}...\")\n",
    "                urllib.request.urlretrieve(sample['url'], path)\n",
    "            paths.append(path)\n",
    "        return paths\n",
    "\n",
    "# Setup environment\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import YOLOv8 detector\n",
    "try:\n",
    "    from src.yolo_detector import YOLOv8Detector\n",
    "except ImportError:\n",
    "    # If import fails, use the Ultralytics YOLO directly\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    class YOLOv8Detector:\n",
    "        def __init__(self, model_size='n', conf=0.25, iou=0.45, device=None):\n",
    "            if device is None:\n",
    "                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            else:\n",
    "                self.device = device\n",
    "                \n",
    "            model_path = f'yolov8{model_size}.pt'\n",
    "            self.model = YOLO(model_path)\n",
    "            \n",
    "            self.conf = conf\n",
    "            self.iou = iou\n",
    "            self.class_names = self.model.names\n",
    "            \n",
    "            print(f\"YOLOv8{model_size} detector initialized on {self.device}\")\n",
    "        \n",
    "        def detect(self, image, show_result=True, return_processed_image=False):\n",
    "            result = self.model(image, conf=self.conf, iou=self.iou)[0]\n",
    "            \n",
    "            if show_result:\n",
    "                im_array = result.plot()\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis('off')\n",
    "                \n",
    "                if isinstance(image, str):\n",
    "                    plt.title(f\"Detection Results: {Path(image).name}\")\n",
    "                else:\n",
    "                    plt.title(f\"Detection Results\")\n",
    "                    \n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nDetections:\")\n",
    "                boxes = result.boxes\n",
    "                for i, box in enumerate(boxes):\n",
    "                    class_id = int(box.cls.item())\n",
    "                    class_name = self.class_names[class_id]\n",
    "                    confidence = box.conf.item()\n",
    "                    print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "            \n",
    "            if return_processed_image:\n",
    "                return result.plot()\n",
    "            \n",
    "            return result\n",
    "\n",
    "# Initialize the YOLOv8 detector\n",
    "detector = YOLOv8Detector(model_size='n', conf=0.25, iou=0.45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Spaces Integration\n",
    "\n",
    "In this section, we'll create a simple web demo using Gradio that can be deployed to Hugging Face Spaces. This allows anyone to use your object detection model through a web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import Gradio for web interface\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_image_for_detection(image, conf_threshold=0.25, iou_threshold=0.45, model_size='n'):\n",
    "    \"\"\"\n",
    "    Process an image for object detection using YOLOv8.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image from Gradio (PIL Image)\n",
    "        conf_threshold: Confidence threshold for detection\n",
    "        iou_threshold: IoU threshold for NMS\n",
    "        model_size: YOLOv8 model size ('n', 's', 'm', 'l', 'x')\n",
    "        \n",
    "    Returns:\n",
    "        Annotated image with detections\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Initialize detector with desired parameters\n",
    "        detector = YOLOv8Detector(\n",
    "            model_size=model_size,\n",
    "            conf=conf_threshold,\n",
    "            iou=iou_threshold\n",
    "        )\n",
    "        \n",
    "        # Run detection\n",
    "        result = detector.detect(img_array, show_result=False)\n",
    "        \n",
    "        # Get processed image with detections\n",
    "        annotated_image = result.plot()\n",
    "        \n",
    "        # Get detection information\n",
    "        boxes = result.boxes\n",
    "        detection_info = \"\"\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            detection_info += f\"Detected {len(boxes)} objects:\\n\"\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                class_id = int(box.cls.item())\n",
    "                class_name = detector.class_names[class_id]\n",
    "                confidence = box.conf.item()\n",
    "                detection_info += f\"{i+1}. {class_name} (Confidence: {confidence:.2f})\\n\"\n",
    "        else:\n",
    "            detection_info = \"No objects detected.\"\n",
    "        \n",
    "        # Convert BGR to RGB for displaying in Gradio\n",
    "        annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        return annotated_image_rgb, detection_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return image, f\"Error processing image: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a Gradio interface\n",
    "with gr.Blocks(title=\"YOLOv8 Object Detection\") as demo:\n",
    "    gr.Markdown(\"# YOLOv8 Object Detection Demo\")\n",
    "    gr.Markdown(\"Upload an image or take a photo to detect objects.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                conf_slider = gr.Slider(minimum=0.1, maximum=0.9, value=0.25, step=0.05, label=\"Confidence Threshold\")\n",
    "                iou_slider = gr.Slider(minimum=0.1, maximum=0.9, value=0.45, step=0.05, label=\"IoU Threshold\")\n",
    "            \n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"n\", \"s\", \"m\", \"l\", \"x\"],\n",
    "                value=\"n\",\n",
    "                label=\"Model Size\",\n",
    "                info=\"n=nano, s=small, m=medium, l=large, x=xlarge\"\n",
    "            )\n",
    "            \n",
    "            detect_button = gr.Button(\"Detect Objects\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_image = gr.Image(type=\"numpy\", label=\"Detection Results\")\n",
    "            output_text = gr.Textbox(label=\"Detection Information\", lines=8)\n",
    "    \n",
    "    # Add examples\n",
    "    gr.Examples(\n",
    "        examples=[\"https://ultralytics.com/images/zidane.jpg\", \"https://ultralytics.com/images/bus.jpg\"],\n",
    "        inputs=input_image,\n",
    "    )\n",
    "    \n",
    "    # Set up button click event\n",
    "    detect_button.click(\n",
    "        fn=process_image_for_detection,\n",
    "        inputs=[input_image, conf_slider, iou_slider, model_dropdown],\n",
    "        outputs=[output_image, output_text]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ### How to Use\n",
    "        1. Upload an image or take a photo using your webcam\n",
    "        2. Adjust the confidence and IoU thresholds if needed\n",
    "        3. Select a model size (larger models are more accurate but slower)\n",
    "        4. Click \"Detect Objects\" to run the detection\n",
    "        \n",
    "        ### About\n",
    "        This demo uses YOLOv8, a state-of-the-art object detection model that can detect 80 different object categories.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=True, inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying to Hugging Face Spaces\n",
    "\n",
    "To deploy this demo to Hugging Face Spaces, you'll need to create the following files:\n",
    "\n",
    "1. `app.py` with the Gradio code\n",
    "2. `requirements.txt` with the dependencies\n",
    "3. Any supporting modules\n",
    "\n",
    "Let's create these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create app.py\n",
    "app_py = \"\"\"\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Helper class for YOLOv8 detection\n",
    "class YOLOv8Detector:\n",
    "    def __init__(self, model_size='n', conf=0.25, iou=0.45, device=None):\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        model_path = f'yolov8{model_size}.pt'\n",
    "        self.model = YOLO(model_path)\n",
    "        \n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "        self.class_names = self.model.names\n",
    "        \n",
    "        print(f\"YOLOv8{model_size} detector initialized on {self.device}\")\n",
    "    \n",
    "    def detect(self, image, show_result=False):\n",
    "        result = self.model(image, conf=self.conf, iou=self.iou)[0]\n",
    "        return result\n",
    "\n",
    "def process_image_for_detection(image, conf_threshold=0.25, iou_threshold=0.45, model_size='n'):\n",
    "    \"\"\"Process an image for object detection using YOLOv8.\"\"\"\n",
    "    try:\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Initialize detector with desired parameters\n",
    "        detector = YOLOv8Detector(\n",
    "            model_size=model_size,\n",
    "            conf=conf_threshold,\n",
    "            iou=iou_threshold\n",
    "        )\n",
    "        \n",
    "        # Run detection\n",
    "        result = detector.detect(img_array)\n",
    "        \n",
    "        # Get processed image with detections\n",
    "        annotated_image = result.plot()\n",
    "        \n",
    "        # Get detection information\n",
    "        boxes = result.boxes\n",
    "        detection_info = \"\"\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            detection_info += f\"Detected {len(boxes)} objects:\\n\"\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                class_id = int(box.cls.item())\n",
    "                class_name = detector.class_names[class_id]\n",
    "                confidence = box.conf.item()\n",
    "                detection_info += f\"{i+1}. {class_name} (Confidence: {confidence:.2f})\\n\"\n",
    "        else:\n",
    "            detection_info = \"No objects detected.\"\n",
    "        \n",
    "        # Convert BGR to RGB for displaying in Gradio\n",
    "        annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        return annotated_image_rgb, detection_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return image, f\"Error processing image: {str(e)}\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "with gr.Blocks(title=\"YOLOv8 Object Detection\") as demo:\n",
    "    gr.Markdown(\"# YOLOv8 Object Detection Demo\")\n",
    "    gr.Markdown(\"Upload an image or take a photo to detect objects.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n",
    "            \n",
    "            with gr.Row():\n",
    "                conf_slider = gr.Slider(minimum=0.1, maximum=0.9, value=0.25, step=0.05, label=\"Confidence Threshold\")\n",
    "                iou_slider = gr.Slider(minimum=0.1, maximum=0.9, value=0.45, step=0.05, label=\"IoU Threshold\")\n",
    "            \n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"n\", \"s\", \"m\", \"l\", \"x\"],\n",
    "                value=\"n\",\n",
    "                label=\"Model Size\",\n",
    "                info=\"n=nano, s=small, m=medium, l=large, x=xlarge\"\n",
    "            )\n",
    "            \n",
    "            detect_button = gr.Button(\"Detect Objects\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_image = gr.Image(type=\"numpy\", label=\"Detection Results\")\n",
    "            output_text = gr.Textbox(label=\"Detection Information\", lines=8)\n",
    "    \n",
    "    # Add examples\n",
    "    gr.Examples(\n",
    "        examples=[\"https://ultralytics.com/images/zidane.jpg\", \"https://ultralytics.com/images/bus.jpg\"],\n",
    "        inputs=input_image,\n",
    "    )\n",
    "    \n",
    "    # Set up button click event\n",
    "    detect_button.click(\n",
    "        fn=process_image_for_detection,\n",
    "        inputs=[input_image, conf_slider, iou_slider, model_dropdown],\n",
    "        outputs=[output_image, output_text]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ### How to Use\n",
    "        1. Upload an image or take a photo using your webcam\n",
    "        2. Adjust the confidence and IoU thresholds if needed\n",
    "        3. Select a model size (larger models are more accurate but slower)\n",
    "        4. Click \"Detect Objects\" to run the detection\n",
    "        \n",
    "        ### About\n",
    "        This demo uses YOLOv8, a state-of-the-art object detection model that can detect 80 different object categories.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch()\n",
    "\"\"\"\n",
    "\n",
    "# Create requirements.txt\n",
    "requirements_txt = \"\"\"\n",
    "ultralytics>=8.0.0\n",
    "gradio>=3.50.0\n",
    "torch>=1.7.0\n",
    "opencv-python>=4.5.0\n",
    "numpy>=1.20.0\n",
    "Pillow>=8.0.0\n",
    "\"\"\"\n",
    "\n",
    "# Create README.md\n",
    "readme_md = \"\"\"\n",
    "# YOLOv8 Object Detection Demo\n",
    "\n",
    "This Space demonstrates object detection using YOLOv8, a state-of-the-art object detection model.\n",
    "\n",
    "## How to Use\n",
    "\n",
    "1. Upload an image or take a photo using your webcam\n",
    "2. Adjust the confidence and IoU thresholds if needed\n",
    "3. Select a model size (larger models are more accurate but slower)\n",
    "4. Click \"Detect Objects\" to run the detection\n",
    "\n",
    "## About YOLOv8\n",
    "\n",
    "YOLOv8 is the latest version in the YOLO (You Only Look Once) family of models. It can detect 80 different object categories with high accuracy and speed.\n",
    "\n",
    "## Model Sizes\n",
    "\n",
    "- **YOLOv8n**: Nano model (smallest, fastest, least accurate)\n",
    "- **YOLOv8s**: Small model\n",
    "- **YOLOv8m**: Medium model\n",
    "- **YOLOv8l**: Large model\n",
    "- **YOLOv8x**: Extra-large model (largest, slowest, most accurate)\n",
    "\"\"\"\n",
    "\n",
    "# Save these files to disk\n",
    "os.makedirs('huggingface_demo', exist_ok=True)\n",
    "\n",
    "with open('huggingface_demo/app.py', 'w') as f:\n",
    "    f.write(app_py)\n",
    "\n",
    "with open('huggingface_demo/requirements.txt', 'w') as f:\n",
    "    f.write(requirements_txt)\n",
    "\n",
    "with open('huggingface_demo/README.md', 'w') as f:\n",
    "    f.write(readme_md)\n",
    "\n",
    "print(\"Created files for Hugging Face Spaces deployment in 'huggingface_demo' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration with PyTorch Lightning\n",
    "\n",
    "PyTorch Lightning provides a more structured way to organize PyTorch code. Here's how to integrate YOLOv8 with PyTorch Lightning for a more organized approach to object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a PyTorch Lightning module for YOLOv8\n",
    "class YOLOv8Module(pl.LightningModule):\n",
    "    def __init__(self, model_size='n', conf=0.25, iou=0.45):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize YOLOv8 detector\n",
    "        self.detector = YOLOv8Detector(\n",
    "            model_size=model_size,\n",
    "            conf=conf,\n",
    "            iou=iou\n",
    "        )\n",
    "        \n",
    "        # Store the class names\n",
    "        self.class_names = self.detector.class_names\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the model\n",
    "        return self.detector.model(x)\n",
    "    \n",
    "    def detect_objects(self, image):\n",
    "        # Detect objects in an image\n",
    "        return self.detector.detect(image, show_result=False)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # This would be used for training\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # Process a batch of images\n",
    "        return self(batch)\n",
    "\n",
    "# Create a simple dataset for object detection\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        \n",
    "        return img, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import albumentations for data augmentation\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Create a transform for preprocessing images\n",
    "transform = A.Compose([\n",
    "    A.Resize(640, 640),  # Resize to YOLOv8 input size\n",
    "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),  # Normalize for YOLOv8\n",
    "    ToTensorV2(),  # Convert to tensor\n",
    "])\n",
    "\n",
    "# Download sample images\n",
    "image_paths = download_sample_images()\n",
    "\n",
    "# Create a dataset\n",
    "dataset = ObjectDetectionDataset(image_paths, transform=transform)\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "# Initialize the PyTorch Lightning module\n",
    "model = YOLOv8Module(model_size='n', conf=0.25, iou=0.45)\n",
    "\n",
    "# Create a trainer\n",
    "trainer = pl.Trainer(max_epochs=1, accelerator='auto')\n",
    "\n",
    "# Show how to predict with the trainer\n",
    "print(\"\\nPredicting with PyTorch Lightning:\")\n",
    "predictions = trainer.predict(model, dataloader)\n",
    "print(f\"Received {len(predictions)} batches of predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Now let's use our YOLOv8Module to detect objects in images\n",
    "for image_path in image_paths:\n",
    "    print(f\"\\nDetecting objects in {image_path}\")\n",
    "    result = model.detect_objects(image_path)\n",
    "    \n",
    "    # Display the result\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Detection Results: {Path(image_path).name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection results\n",
    "    boxes = result.boxes\n",
    "    print(f\"Detected {len(boxes)} objects:\")\n",
    "    for i, box in enumerate(boxes):\n",
    "        class_id = int(box.cls.item())\n",
    "        class_name = model.class_names[class_id]\n",
    "        confidence = box.conf.item()\n",
    "        print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Different Image Formats and Sources\n",
    "\n",
    "YOLOv8 can work with various image formats and sources. Let's explore how to handle different types of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detect_from_various_sources(detector, sources):\n",
    "    \"\"\"Demonstrate object detection from various image sources.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for source in sources:\n",
    "        print(f\"\\nProcessing source: {source['type']}\")\n",
    "        \n",
    "        # Process based on source type\n",
    "        if source['type'] == 'file':\n",
    "            # Local file\n",
    "            image_path = source['path']\n",
    "            result = detector.detect(image_path, show_result=False)\n",
    "            \n",
    "            # Display the result\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"File: {Path(image_path).name}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        elif source['type'] == 'url':\n",
    "            # Image from URL\n",
    "            url = source['url']\n",
    "            try:\n",
    "                # Download image\n",
    "                response = requests.get(url)\n",
    "                img = Image.open(BytesIO(response.content))\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Convert RGB to BGR for OpenCV compatibility\n",
    "                if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n",
    "                    img_array = img_array[:, :, ::-1].copy()\n",
    "                \n",
    "                # Detect objects\n",
    "                result = detector.detect(img_array, show_result=False)\n",
    "                \n",
    "                # Display the result\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n",
    "                plt.title(f\"URL: {url.split('/')[-1]}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        elif source['type'] == 'base64':\n",
    "            # Base64 encoded image\n",
    "            base64_data = source['data']\n",
    "            try:\n",
    "                # Decode base64 data\n",
    "                img_data = base64.b64decode(base64_data)\n",
    "                img = Image.open(BytesIO(img_data))\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Convert RGB to BGR for OpenCV compatibility\n",
    "                if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n",
    "                    img_array = img_array[:, :, ::-1].copy()\n",
    "                \n",
    "                # Detect objects\n",
    "                result = detector.detect(img_array, show_result=False)\n",
    "                \n",
    "                # Display the result\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n",
    "                plt.title(\"Base64 Image\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing base64 image: {e}\")\n",
    "                continue\n",
    "        \n",
    "        elif source['type'] == 'numpy':\n",
    "            # Numpy array\n",
    "            img_array = source['array']\n",
    "            \n",
    "            # Detect objects\n",
    "            result = detector.detect(img_array, show_result=False)\n",
    "            \n",
    "            # Display the result\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Numpy Array Image\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unknown source type: {source['type']}\")\n",
    "            continue\n",
    "        \n",
    "        # Print detection summary\n",
    "        boxes = result.boxes\n",
    "        print(f\"Detected {len(boxes)} objects:\")\n",
    "        for i, box in enumerate(boxes):\n",
    "            class_id = int(box.cls.item())\n",
    "            class_name = detector.class_names[class_id]\n",
    "            confidence = box.conf.item()\n",
    "            print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sample sources\n",
    "sources = []\n",
    "\n",
    "# 1. Local file\n",
    "if image_paths:\n",
    "    sources.append({\n",
    "        'type': 'file',\n",
    "        'path': image_paths[0]\n",
    "    })\n",
    "\n",
    "# 2. URL\n",
    "sources.append({\n",
    "    'type': 'url',\n",
    "    'url': 'https://raw.githubusercontent.com/ultralytics/assets/main/im/image3.jpg'\n",
    "})\n",
    "\n",
    "# 3. Numpy array (create a simple test image)\n",
    "test_image = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "test_image[50:250, 50:250, :] = [0, 0, 255]  # Draw a red square\n",
    "sources.append({\n",
    "    'type': 'numpy',\n",
    "    'array': test_image\n",
    "})\n",
    "\n",
    "# Process different sources\n",
    "detect_from_various_sources(detector, sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with Computer Vision Pipelines\n",
    "\n",
    "YOLOv8 can be integrated into larger computer vision pipelines. Here's an example of combining object detection with additional processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ComputerVisionPipeline:\n",
    "    \"\"\"A pipeline that combines object detection with other CV tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector=None):\n",
    "        \"\"\"Initialize the pipeline.\"\"\"\n",
    "        # Initialize the object detector\n",
    "        if detector is None:\n",
    "            self.detector = YOLOv8Detector(model_size='n', conf=0.25, iou=0.45)\n",
    "        else:\n",
    "            self.detector = detector\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess the input image.\"\"\"\n",
    "        # Convert to numpy array if needed\n",
    "        if isinstance(image, str):\n",
    "            # Load from file path\n",
    "            img = cv2.imread(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        elif isinstance(image, Image.Image):\n",
    "            # Convert PIL image to numpy array\n",
    "            img = np.array(image)\n",
    "        else:\n",
    "            # Assume it's already a numpy array\n",
    "            img = image.copy()\n",
    "        \n",
    "        # Ensure image is in RGB format\n",
    "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "            if np.max(img) <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)  # Normalize to 0-255 if needed\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def detect_objects(self, image):\n",
    "        \"\"\"Detect objects in the image.\"\"\"\n",
    "        return self.detector.detect(image, show_result=False)\n",
    "    \n",
    "    def process_detections(self, image, result):\n",
    "        \"\"\"Process detection results for further analysis.\"\"\"\n",
    "        # Get bounding boxes\n",
    "        boxes = result.boxes\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        processed_results = {\n",
    "            'objects': [],\n",
    "            'object_count': len(boxes),\n",
    "            'class_counts': {},\n",
    "            'largest_object': None,\n",
    "            'average_confidence': 0.0\n",
    "        }\n",
    "        \n",
    "        # Extract detection information\n",
    "        confidences = []\n",
    "        largest_area = 0\n",
    "        largest_object_idx = -1\n",
    "        \n",
    "        for i, box in enumerate(boxes):\n",
    "            # Get class information\n",
    "            class_id = int(box.cls.item())\n",
    "            class_name = self.detector.class_names[class_id]\n",
    "            confidence = float(box.conf.item())\n",
    "            \n",
    "            # Get bounding box coordinates\n",
    "            bbox = box.xyxy[0].tolist()  # xyxy format is [x1, y1, x2, y2]\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            area = width * height\n",
    "            \n",
    "            # Check if this is the largest object\n",
    "            if area > largest_area:\n",
    "                largest_area = area\n",
    "                largest_object_idx = i\n",
    "            \n",
    "            # Extract object region of interest\n",
    "            roi = image[int(y1):int(y2), int(x1):int(x2)].copy()\n",
    "            \n",
    "            # Add to object list\n",
    "            processed_results['objects'].append({\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_name,\n",
    "                'confidence': confidence,\n",
    "                'bbox': bbox,\n",
    "                'area': area,\n",
    "                'roi': roi\n",
    "            })\n",
    "            \n",
    "            # Update class counts\n",
    "            if class_name in processed_results['class_counts']:\n",
    "                processed_results['class_counts'][class_name] += 1\n",
    "            else:\n",
    "                processed_results['class_counts'][class_name] = 1\n",
    "            \n",
    "            # Add to confidence list\n",
    "            confidences.append(confidence)\n",
    "        \n",
    "        # Set largest object\n",
    "        if largest_object_idx >= 0:\n",
    "            processed_results['largest_object'] = processed_results['objects'][largest_object_idx]\n",
    "        \n",
    "        # Calculate average confidence\n",
    "        if confidences:\n",
    "            processed_results['average_confidence'] = sum(confidences) / len(confidences)\n",
    "        \n",
    "        return processed_results\n",
    "    \n",
    "    def visualize_results(self, image, processed_results):\n",
    "        \"\"\"Visualize the processed results.\"\"\"\n",
    "        # Create a copy of the image for visualization\n",
    "        vis_image = image.copy()\n",
    "        \n",
    "        # Draw bounding boxes and labels for all objects\n",
    "        for obj in processed_results['objects']:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = obj['bbox']\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            \n",
    "            # Get class name and confidence\n",
    "            class_name = obj['class_name']\n",
    "            confidence = obj['confidence']\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(vis_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw label background\n",
    "            text = f\"{class_name} {confidence:.2f}\"\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            cv2.rectangle(vis_image, (x1, y1 - text_size[1] - 5), (x1 + text_size[0], y1), (0, 255, 0), -1)\n",
    "            \n",
    "            # Draw label text\n",
    "            cv2.putText(vis_image, text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "        \n",
    "        # Draw summary information\n",
    "        summary_text = [\n",
    "            f\"Total Objects: {processed_results['object_count']}\",\n",
    "            f\"Avg Confidence: {processed_results['average_confidence']:.2f}\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(summary_text):\n",
    "            y_pos = 30 + i * 30\n",
    "            cv2.putText(vis_image, text, (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        \n",
    "        return vis_image\n",
    "    \n",
    "    def analyze_scene(self, image):\n",
    "        \"\"\"Analyze the scene for objects and their relationships.\"\"\"\n",
    "        # Preprocess the image\n",
    "        processed_image = self.preprocess_image(image)\n",
    "        \n",
    "        # Detect objects\n",
    "        detection_result = self.detect_objects(processed_image)\n",
    "        \n",
    "        # Process detections\n",
    "        processed_results = self.process_detections(processed_image, detection_result)\n",
    "        \n",
    "        # Visualize results\n",
    "        visualization = self.visualize_results(processed_image, processed_results)\n",
    "        \n",
    "        # Display results\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(visualization)\n",
    "        plt.title(\"Scene Analysis Results\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print analysis summary\n",
    "        print(\"\\nScene Analysis Summary:\")\n",
    "        print(f\"Total objects detected: {processed_results['object_count']}\")\n",
    "        print(f\"Average confidence: {processed_results['average_confidence']:.2f}\")\n",
    "        \n",
    "        print(\"\\nObject classes:\")\n",
    "        for class_name, count in processed_results['class_counts'].items():\n",
    "            print(f\"  {class_name}: {count}\")\n",
    "        \n",
    "        if processed_results['largest_object']:\n",
    "            largest = processed_results['largest_object']\n",
    "            print(f\"\\nLargest object: {largest['class_name']} (Area: {largest['area']:.1f} pixelsÂ²)\")\n",
    "            \n",
    "            # Display the largest object ROI\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            plt.imshow(largest['roi'])\n",
    "            plt.title(f\"Largest Object: {largest['class_name']}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        return processed_results, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a computer vision pipeline\n",
    "pipeline = ComputerVisionPipeline(detector)\n",
    "\n",
    "# Test the pipeline on a sample image\n",
    "if image_paths:\n",
    "    sample_image = image_paths[0]\n",
    "    print(f\"\\nAnalyzing scene in {sample_image}...\")\n",
    "    processed_results, visualization = pipeline.analyze_scene(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated several ways to integrate YOLOv8 object detection functionality with popular frameworks and applications:\n",
    "\n",
    "1. **Hugging Face Spaces Integration**: We created a web demo using Gradio that can be deployed to Hugging Face Spaces, allowing anyone to use the model through a web interface.\n",
    "\n",
    "2. **PyTorch Lightning Integration**: We showed how to integrate YOLOv8 with PyTorch Lightning for a more structured approach to object detection, which would be particularly useful for training and fine-tuning workflows.\n",
    "\n",
    "3. **Working with Different Image Formats**: We demonstrated how to handle various image sources, including local files, URLs, base64-encoded images, and numpy arrays.\n",
    "\n",
    "4. **Computer Vision Pipelines**: We created a comprehensive pipeline that combines object detection with additional analysis and visualization steps.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore real-time object detection with a webcam feed, which will complete our set of core functionalities for the YOLOv8 object detection project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}