{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained YOLOv8 Model Integration\n",
    "\n",
    "This notebook demonstrates how to load and use pre-trained YOLOv8 models for object detection using the Ultralytics implementation. We'll configure the models for inference and ensure they're ready for various applications.\n",
    "\n",
    "**Contents:**\n",
    "1. Setting up the environment\n",
    "2. Loading pre-trained YOLOv8 models\n",
    "3. Understanding model architecture and properties\n",
    "4. Basic inference with sample images\n",
    "5. Adjusting detection parameters\n",
    "6. Exporting models to different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "First, let's install the necessary packages for working with YOLOv8. The primary package is `ultralytics`, which provides the YOLOv8 implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if running in Colab or Kaggle\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'kaggle_secrets' in sys.modules\n",
    "\n",
    "if IN_COLAB or IN_KAGGLE:\n",
    "    print(f\"Running in {'Google Colab' if IN_COLAB else 'Kaggle'}, installing dependencies...\")\n",
    "    !pip install -q ultralytics\n",
    "    !pip install -q opencv-python matplotlib\n",
    "    !pip install -q torch torchvision\n",
    "    !pip install -q ipywidgets\n",
    "else:\n",
    "    print(\"Not running in Colab or Kaggle. If needed, install dependencies manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Set up matplotlib for inline plotting\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check system info and GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Pre-trained YOLOv8 Models\n",
    "\n",
    "YOLOv8 comes with several pre-trained models of different sizes, optimized for different scenarios:\n",
    "\n",
    "- **YOLOv8n**: Nano model (smallest, fastest, least accurate)\n",
    "- **YOLOv8s**: Small model\n",
    "- **YOLOv8m**: Medium model\n",
    "- **YOLOv8l**: Large model\n",
    "- **YOLOv8x**: Extra-large model (largest, slowest, most accurate)\n",
    "\n",
    "Let's load a few of these models to see how they compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the YOLOv8 nano model\n",
    "model_nano = YOLO('yolov8n.pt')\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model: YOLOv8n\")\n",
    "print(f\"Task: {model_nano.task}\")\n",
    "print(f\"Number of classes: {len(model_nano.names)}\")\n",
    "print(f\"Input size: {model_nano.model.args['imgsz']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# View model class names\n",
    "print(\"COCO Classes detected by YOLOv8:\")\n",
    "for idx, (class_id, class_name) in enumerate(model_nano.names.items()):\n",
    "    print(f\"{class_id}: {class_name}\", end=\"\\t\")\n",
    "    if (idx + 1) % 5 == 0:  # Print 5 classes per line\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to load different YOLOv8 models\n",
    "def load_yolo_model(model_size='n'):\n",
    "    \"\"\"Load a YOLOv8 model of specified size\n",
    "    \n",
    "    Args:\n",
    "        model_size (str): Model size (n, s, m, l, x)\n",
    "        \n",
    "    Returns:\n",
    "        YOLO model instance\n",
    "    \"\"\"\n",
    "    model_path = f'yolov8{model_size}.pt'\n",
    "    model = YOLO(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Let's compare the model sizes\n",
    "model_sizes = ['n', 's', 'm']\n",
    "models = {}\n",
    "\n",
    "print(\"Loading models of different sizes...\")\n",
    "for size in model_sizes:\n",
    "    print(f\"Loading YOLOv8{size}...\")\n",
    "    models[size] = load_yolo_model(size)\n",
    "    \n",
    "print(\"\\nModel comparison:\")\n",
    "for size in model_sizes:\n",
    "    model = models[size]\n",
    "    # Count model parameters\n",
    "    num_params = sum(p.numel() for p in model.model.parameters())\n",
    "    print(f\"YOLOv8{size}: {num_params/1e6:.2f} million parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Model Architecture and Properties\n",
    "\n",
    "Let's examine the YOLOv8 model architecture to understand how it works under the hood. YOLOv8 consists of:\n",
    "\n",
    "1. **Backbone**: Extracts features from the input image (CSPDarknet based)\n",
    "2. **Neck**: Aggregates features from different levels (Feature Pyramid Network)\n",
    "3. **Head**: Makes predictions (bounding boxes, class probabilities, objectness scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Examine model structure - this will show the model summary with layer information\n",
    "model = models['n']  # Use the nano model for illustration\n",
    "print(model.model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze the model's PyTorch layers\n",
    "def analyze_model_layers(model):\n",
    "    \"\"\"Analyze and print information about model layers\"\"\"\n",
    "    # Get the underlying PyTorch model\n",
    "    torch_model = model.model\n",
    "    \n",
    "    # Count different types of layers\n",
    "    layer_counts = {}\n",
    "    total_params = 0\n",
    "    trainable_params = 0\n",
    "    \n",
    "    for name, module in torch_model.named_modules():\n",
    "        class_name = module.__class__.__name__\n",
    "        if class_name in layer_counts:\n",
    "            layer_counts[class_name] += 1\n",
    "        else:\n",
    "            layer_counts[class_name] = 1\n",
    "        \n",
    "        # Count parameters\n",
    "        for param in module.parameters(recurse=False):\n",
    "            total_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "    \n",
    "    print(f\"Model Layer Analysis:\\n\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"\\nLayer types:\")\n",
    "    for layer_type, count in sorted(layer_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {layer_type}: {count}\")\n",
    "\n",
    "# Run the analysis on the nano model\n",
    "analyze_model_layers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv8 uses a modular architecture with components like:\n",
    "\n",
    "- **Conv**: Standard convolutional layers for feature extraction\n",
    "- **C2f**: Cross-stage partial bottleneck with 2 convolutions and faster inference\n",
    "- **SPPF**: Spatial Pyramid Pooling - Fast layer for extracting features at different scales\n",
    "- **Detect**: Detection head that produces the final predictions\n",
    "\n",
    "Each model size (n, s, m, l, x) uses the same architecture but with different scaling factors for width and depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Inference with Sample Images\n",
    "\n",
    "Now let's perform object detection using the pre-trained YOLOv8 model on some sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a directory for saving test images if it doesn't exist\n",
    "os.makedirs('test_images', exist_ok=True)\n",
    "\n",
    "# Function to download an image from URL\n",
    "def download_image(url, save_path=None):\n",
    "    \"\"\"Download an image from URL and return it as a numpy array\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    \n",
    "    # Save image if path is provided\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        img.save(save_path)\n",
    "        print(f\"Image saved to {save_path}\")\n",
    "    \n",
    "    # Convert PIL image to numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert RGB to BGR for OpenCV compatibility\n",
    "    if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n",
    "        img_array = img_array[:, :, ::-1]\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample image URLs for testing\n",
    "test_images = [\n",
    "    {'name': 'street_scene', 'url': 'https://ultralytics.com/images/zidane.jpg'},\n",
    "    {'name': 'bus', 'url': 'https://ultralytics.com/images/bus.jpg'},\n",
    "    {'name': 'people', 'url': 'https://raw.githubusercontent.com/ultralytics/assets/main/im/image2.jpg'}\n",
    "]\n",
    "\n",
    "# Download test images\n",
    "image_paths = []\n",
    "for img_info in test_images:\n",
    "    image_path = f\"test_images/{img_info['name']}.jpg\"\n",
    "    download_image(img_info['url'], image_path)\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform inference on the test images\n",
    "model = models['n']  # Use the nano model for inference\n",
    "\n",
    "# List to store detection results\n",
    "results = []\n",
    "\n",
    "# Process each image\n",
    "for image_path in image_paths:\n",
    "    # Run inference\n",
    "    result = model(image_path)\n",
    "    results.append(result[0])  # First element is the result for the first image\n",
    "    \n",
    "    # Print detection summary\n",
    "    print(f\"\\nDetections in {image_path}:\")\n",
    "    boxes = result[0].boxes\n",
    "    for i, box in enumerate(boxes):\n",
    "        class_id = int(box.cls.item())\n",
    "        class_name = model.names[class_id]\n",
    "        confidence = box.conf.item()\n",
    "        bbox = box.xyxy[0].tolist()  # xyxy format is [x1, y1, x2, y2]\n",
    "        \n",
    "        print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "        print(f\"     Bounding box: {[round(coord, 2) for coord in bbox]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize detection results\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    # Create a subplot\n",
    "    plt.subplot(len(results), 1, i+1)\n",
    "    \n",
    "    # Plot the result\n",
    "    im_array = result.plot()\n",
    "    plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Detection Results: {image_paths[i]}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adjusting Detection Parameters\n",
    "\n",
    "YOLOv8 models allow us to adjust various parameters to tune the detection process. Let's explore some key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to perform inference with different confidence thresholds\n",
    "def inference_with_conf_threshold(model, image_path, conf_thresholds=[0.25, 0.5, 0.75]):\n",
    "    \"\"\"\n",
    "    Perform inference with different confidence thresholds and visualize results\n",
    "    \n",
    "    Args:\n",
    "        model: YOLOv8 model\n",
    "        image_path: Path to the input image\n",
    "        conf_thresholds: List of confidence thresholds to test\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 5 * len(conf_thresholds)))\n",
    "    \n",
    "    for i, conf in enumerate(conf_thresholds):\n",
    "        # Run inference with specific confidence threshold\n",
    "        result = model(image_path, conf=conf)[0]\n",
    "        \n",
    "        # Plot result\n",
    "        plt.subplot(len(conf_thresholds), 1, i+1)\n",
    "        im_array = result.plot()\n",
    "        plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Calculate number of detections\n",
    "        num_detections = len(result.boxes)\n",
    "        plt.title(f\"Confidence Threshold: {conf} | Detections: {num_detections}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different confidence thresholds on one of the images\n",
    "inference_with_conf_threshold(model, image_paths[0], [0.1, 0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to perform inference with different IoU thresholds\n",
    "def inference_with_iou_threshold(model, image_path, iou_thresholds=[0.45, 0.7, 0.9]):\n",
    "    \"\"\"\n",
    "    Perform inference with different IoU thresholds and visualize results\n",
    "    \n",
    "    Args:\n",
    "        model: YOLOv8 model\n",
    "        image_path: Path to the input image\n",
    "        iou_thresholds: List of IoU thresholds to test\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 5 * len(iou_thresholds)))\n",
    "    \n",
    "    for i, iou in enumerate(iou_thresholds):\n",
    "        # Run inference with specific IoU threshold\n",
    "        result = model(image_path, iou=iou)[0]\n",
    "        \n",
    "        # Plot result\n",
    "        plt.subplot(len(iou_thresholds), 1, i+1)\n",
    "        im_array = result.plot()\n",
    "        plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Calculate number of detections\n",
    "        num_detections = len(result.boxes)\n",
    "        plt.title(f\"IoU Threshold: {iou} | Detections: {num_detections}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test different IoU thresholds on one of the images\n",
    "inference_with_iou_threshold(model, image_paths[1], [0.25, 0.45, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to compare different model sizes on the same image\n",
    "def compare_model_sizes(models, image_path):\n",
    "    \"\"\"\n",
    "    Compare detection results from different model sizes on the same image\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of YOLOv8 models of different sizes\n",
    "        image_path: Path to the input image\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 5 * len(models)))\n",
    "    \n",
    "    for i, (size, model) in enumerate(models.items()):\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        result = model(image_path)[0]\n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        # Plot result\n",
    "        plt.subplot(len(models), 1, i+1)\n",
    "        im_array = result.plot()\n",
    "        plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Calculate number of detections\n",
    "        num_detections = len(result.boxes)\n",
    "        plt.title(f\"YOLOv8{size} | Detections: {num_detections} | Inference time: {inference_time:.3f}s\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare different model sizes on one of the images\n",
    "compare_model_sizes(models, image_paths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exporting Models to Different Formats\n",
    "\n",
    "YOLOv8 models can be exported to various formats for deployment on different platforms. The Ultralytics framework makes this process straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directory for exported models\n",
    "os.makedirs('exported_models', exist_ok=True)\n",
    "\n",
    "# Define a function to export models\n",
    "def export_model(model, format='onnx', imgsz=640):\n",
    "    \"\"\"\n",
    "    Export YOLOv8 model to specified format\n",
    "    \n",
    "    Args:\n",
    "        model: YOLOv8 model\n",
    "        format: Export format (onnx, torchscript, openvino, etc.)\n",
    "        imgsz: Image size for export\n",
    "        \n",
    "    Returns:\n",
    "        Path to exported model\n",
    "    \"\"\"\n",
    "    # Get model size from model\n",
    "    model_type = model.ckpt_path.stem\n",
    "    \n",
    "    # Export the model\n",
    "    export_path = f\"exported_models/{model_type}_{format}\"\n",
    "    \n",
    "    # Use the export method from the YOLO class\n",
    "    result = model.export(format=format, imgsz=imgsz)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export the nano model to ONNX format\n",
    "try:\n",
    "    export_model(models['n'], format='onnx')\n",
    "    print(\"Model exported successfully to ONNX format\")\n",
    "except Exception as e:\n",
    "    print(f\"Error exporting model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Creating a Reusable YOLOv8 Detector Class\n",
    "\n",
    "Let's create a reusable class that encapsulates YOLOv8 functionality for easy use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class YOLOv8Detector:\n",
    "    \"\"\"A class for YOLOv8 object detection\"\"\"\n",
    "    \n",
    "    def __init__(self, model_size='n', conf=0.25, iou=0.45, device=None):\n",
    "        \"\"\"\n",
    "        Initialize YOLOv8 detector\n",
    "        \n",
    "        Args:\n",
    "            model_size (str): YOLOv8 model size ('n', 's', 'm', 'l', 'x')\n",
    "            conf (float): Confidence threshold for detections\n",
    "            iou (float): IoU threshold for NMS\n",
    "            device (str): Device to use ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        # Set device\n",
    "        if device is None:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        # Load model\n",
    "        model_path = f'yolov8{model_size}.pt'\n",
    "        self.model = YOLO(model_path)\n",
    "        \n",
    "        # Set detection parameters\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "        \n",
    "        # Store class names\n",
    "        self.class_names = self.model.names\n",
    "        \n",
    "        print(f\"YOLOv8{model_size} detector initialized on {self.device}\")\n",
    "        print(f\"Confidence threshold: {self.conf}, IoU threshold: {self.iou}\")\n",
    "        \n",
    "    def detect(self, image_path, show_result=True):\n",
    "        \"\"\"\n",
    "        Perform object detection on an image\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to the input image\n",
    "            show_result (bool): Whether to display the result\n",
    "            \n",
    "        Returns:\n",
    "            Results object containing detections\n",
    "        \"\"\"\n",
    "        # Run inference\n",
    "        result = self.model(image_path, conf=self.conf, iou=self.iou)[0]\n",
    "        \n",
    "        # Display result if requested\n",
    "        if show_result:\n",
    "            im_array = result.plot()\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Detection Results: {Path(image_path).name}\")\n",
    "            plt.show()\n",
    "            \n",
    "            # Print detection summary\n",
    "            print(f\"\\nDetections in {Path(image_path).name}:\")\n",
    "            boxes = result.boxes\n",
    "            for i, box in enumerate(boxes):\n",
    "                class_id = int(box.cls.item())\n",
    "                class_name = self.class_names[class_id]\n",
    "                confidence = box.conf.item()\n",
    "                bbox = box.xyxy[0].tolist()  # xyxy format is [x1, y1, x2, y2]\n",
    "                \n",
    "                print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_video(self, video_path=0, output_path=None, show_result=True):\n",
    "        \"\"\"\n",
    "        Perform object detection on a video\n",
    "        \n",
    "        Args:\n",
    "            video_path (str or int): Path to the input video or webcam index (0 for default camera)\n",
    "            output_path (str): Path to save the output video\n",
    "            show_result (bool): Whether to display the result in a notebook\n",
    "            \n",
    "        Returns:\n",
    "            Path to the output video (if saved)\n",
    "        \"\"\"\n",
    "        # Run inference\n",
    "        results = self.model.predict(source=video_path, conf=self.conf, iou=self.iou, save=output_path is not None)\n",
    "        \n",
    "        # If we're saving the output, get the path\n",
    "        if output_path is not None:\n",
    "            output_path = results[0].save_dir\n",
    "            print(f\"Output video saved to {output_path}\")\n",
    "            \n",
    "        # Return the output path if saved\n",
    "        return output_path if output_path is not None else None\n",
    "    \n",
    "    def export(self, format='onnx', imgsz=640):\n",
    "        \"\"\"\n",
    "        Export the model to specified format\n",
    "        \n",
    "        Args:\n",
    "            format (str): Export format\n",
    "            imgsz (int): Image size for export\n",
    "            \n",
    "        Returns:\n",
    "            Path to exported model\n",
    "        \"\"\"\n",
    "        return self.model.export(format=format, imgsz=imgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a YOLOv8 detector instance\n",
    "detector = YOLOv8Detector(model_size='n', conf=0.25, iou=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the detector on a sample image\n",
    "for image_path in image_paths:\n",
    "    result = detector.detect(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integrating with PyTorch\n",
    "\n",
    "YOLOv8 is built on PyTorch, which means we can easily integrate it with other PyTorch components. Let's explore how to access the underlying PyTorch model for more advanced use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Access the PyTorch model\n",
    "pytorch_model = detector.model.model\n",
    "print(f\"Type of PyTorch model: {type(pytorch_model)}\")\n",
    "\n",
    "# List the top-level modules\n",
    "print(\"\\nTop-level modules:\")\n",
    "for name, module in pytorch_model.named_children():\n",
    "    print(f\"{name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to manually run inference using PyTorch directly\n",
    "def run_inference_pytorch(model, image_path, conf_thresh=0.25, iou_thresh=0.45):\n",
    "    \"\"\"\n",
    "    Run inference using PyTorch model directly\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        image_path: Path to input image\n",
    "        conf_thresh: Confidence threshold\n",
    "        iou_thresh: IoU threshold\n",
    "        \n",
    "    Returns:\n",
    "        Processed image with detections\n",
    "    \"\"\"\n",
    "    # Read and preprocess image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resize and normalize image\n",
    "    input_size = 640\n",
    "    img_resized = cv2.resize(img_rgb, (input_size, input_size))\n",
    "    img_float = img_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    x = torch.from_numpy(img_float).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    # To device\n",
    "    device = next(model.parameters()).device\n",
    "    x = x.to(device)\n",
    "    \n",
    "    # Get detector model from YOLOv8 model\n",
    "    detector_model = model\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        preds = detector_model(x)\n",
    "    \n",
    "    # Process predictions (simplified version)\n",
    "    # In practice, you would need to handle NMS and other post-processing\n",
    "    # Using the YOLO class is much easier for this\n",
    "    print(\"Raw prediction shape:\", preds[0].shape)\n",
    "    \n",
    "    # Let's use the higher-level function for visualization instead\n",
    "    yolo = detector.model  # Use the YOLO model we initialized earlier\n",
    "    result = yolo(image_path)[0]\n",
    "    \n",
    "    # Plot the result\n",
    "    im_array = result.plot()\n",
    "    return im_array\n",
    "\n",
    "# Test the PyTorch inference function\n",
    "try:\n",
    "    im_result = run_inference_pytorch(pytorch_model, image_paths[0])\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(im_result, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"PyTorch Direct Inference Result\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error running direct PyTorch inference: {e}\")\n",
    "    print(\"\\nNote: Direct access to the PyTorch model requires understanding of YOLOv8's internal structure.\")\n",
    "    print(\"Using the high-level YOLO API is recommended for most use cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've explored how to load and use pre-trained YOLOv8 models for object detection. We've covered:\n",
    "\n",
    "1. Setting up the environment for YOLOv8\n",
    "2. Loading pre-trained models of different sizes\n",
    "3. Understanding the model architecture\n",
    "4. Performing basic inference on images\n",
    "5. Adjusting detection parameters like confidence and IoU thresholds\n",
    "6. Exporting models to different formats\n",
    "7. Creating a reusable detector class\n",
    "8. Integrating with PyTorch\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the upcoming notebooks, we'll cover:\n",
    "\n",
    "1. **Image Upload and Detection**: Creating a user interface for uploading images and performing object detection\n",
    "2. **Real-time Object Detection**: Implementing real-time detection using webcam input\n",
    "3. **Advanced Features**: Exploring additional YOLOv8 capabilities and optimizations\n",
    "\n",
    "With the foundation established in this notebook, we're now ready to build more practical applications using YOLOv8!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}