{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Upload and Object Detection with YOLOv8\n",
    "\n",
    "This notebook demonstrates how to perform object detection on uploaded images using pre-trained YOLOv8 models. The notebook is designed to work in both Google Colab and Kaggle environments.\n",
    "\n",
    "**Contents:**\n",
    "1. Setting up the environment\n",
    "2. Importing the YOLOv8 detector\n",
    "3. Uploading and preparing images\n",
    "4. Performing object detection\n",
    "5. Displaying and analyzing results\n",
    "6. Batch processing multiple images\n",
    "7. Advanced options for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the Environment\n",
    "\n",
    "First, we'll set up the environment and install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository if running in Colab/Kaggle\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if we're in Colab or Kaggle\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "IN_KAGGLE = 'kaggle_secrets' in sys.modules\n",
    "\n",
    "# If we're in Colab or Kaggle, set up the environment\n",
    "if IN_COLAB or IN_KAGGLE:\n",
    "    # Clone the repository\n",
    "    !git clone -q https://github.com/yourusername/object-detection-yolo.git\n",
    "    %cd object-detection-yolo\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q ultralytics opencv-python ipywidgets matplotlib Pillow\n",
    "    \n",
    "    # Add the repository root to the Python path\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "    \n",
    "    print(f\"Setting up in {'Google Colab' if IN_COLAB else 'Kaggle'}\")\n",
    "else:\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the utilities for notebook environment\n",
    "try:\n",
    "    from utils.notebook_utils import (\n",
    "        setup_env, show_upload_widget, process_upload_widget_kaggle,\n",
    "        process_upload_colab, display_image_with_info, download_sample_images\n",
    "    )\n",
    "except ImportError:\n",
    "    # If imports fail, create a minimal version of necessary functions\n",
    "    def setup_env():\n",
    "        print(\"Environment setup simplified due to import error\")\n",
    "        if 'google.colab' in sys.modules:\n",
    "            !pip install -q ultralytics opencv-python ipywidgets\n",
    "            from google.colab import files\n",
    "            print(\"Running in Google Colab\")\n",
    "        elif 'kaggle_secrets' in sys.modules:\n",
    "            !pip install -q ultralytics opencv-python ipywidgets\n",
    "            print(\"Running in Kaggle\")\n",
    "        else:\n",
    "            print(\"Running locally\")\n",
    "    \n",
    "    def show_upload_widget():\n",
    "        if 'google.colab' in sys.modules:\n",
    "            from google.colab import files\n",
    "            print(\"Select an image to upload:\")\n",
    "            return files.upload()\n",
    "        else:\n",
    "            try:\n",
    "                import ipywidgets as widgets\n",
    "                from IPython.display import display\n",
    "                file_upload = widgets.FileUpload(\n",
    "                    accept='.jpg,.jpeg,.png',\n",
    "                    multiple=False,\n",
    "                    description='Upload:'\n",
    "                )\n",
    "                display(file_upload)\n",
    "                return file_upload\n",
    "            except ImportError:\n",
    "                print(\"Please install ipywidgets for upload functionality\")\n",
    "                return None\n",
    "    \n",
    "    def process_upload_widget_kaggle(file_upload):\n",
    "        if not file_upload.value:\n",
    "            return []\n",
    "        \n",
    "        import os\n",
    "        os.makedirs('uploads', exist_ok=True)\n",
    "        \n",
    "        paths = []\n",
    "        for name, file_info in file_upload.value.items():\n",
    "            path = f'uploads/{name}'\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(file_info['content'])\n",
    "            paths.append(path)\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def process_upload_colab(uploaded_files):\n",
    "        if not uploaded_files:\n",
    "            return []\n",
    "        \n",
    "        import os\n",
    "        os.makedirs('uploads', exist_ok=True)\n",
    "        \n",
    "        paths = []\n",
    "        for name, content in uploaded_files.items():\n",
    "            path = f'uploads/{name}'\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(content)\n",
    "            paths.append(path)\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def display_image_with_info(image_path):\n",
    "        import cv2\n",
    "        import matplotlib.pyplot as plt\n",
    "        from pathlib import Path\n",
    "        \n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        print(f\"Image: {Path(image_path).name}\")\n",
    "        print(f\"Resolution: {img.shape[1]}x{img.shape[0]}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(Path(image_path).name)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    def download_sample_images():\n",
    "        import os\n",
    "        import urllib.request\n",
    "        from pathlib import Path\n",
    "        \n",
    "        sample_dir = Path('sample_images')\n",
    "        sample_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        samples = [\n",
    "            {'url': 'https://ultralytics.com/images/zidane.jpg', 'name': 'person.jpg'},\n",
    "            {'url': 'https://ultralytics.com/images/bus.jpg', 'name': 'bus.jpg'}\n",
    "        ]\n",
    "        \n",
    "        paths = []\n",
    "        for sample in samples:\n",
    "            path = sample_dir / sample['name']\n",
    "            if not path.exists():\n",
    "                print(f\"Downloading {sample['name']}...\")\n",
    "                urllib.request.urlretrieve(sample['url'], path)\n",
    "            paths.append(str(path))\n",
    "        \n",
    "        return paths\n",
    "\n",
    "# Set up the environment\n",
    "setup_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing the YOLOv8 Detector\n",
    "\n",
    "Now, let's import the YOLOv8 detector class that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import YOLOv8 detector\n",
    "try:\n",
    "    from src.yolo_detector import YOLOv8Detector\n",
    "except ImportError:\n",
    "    # If the import fails, define a minimal version here\n",
    "    from ultralytics import YOLO\n",
    "    import torch\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    import time\n",
    "    \n",
    "    class YOLOv8Detector:\n",
    "        def __init__(self, model_size='n', conf=0.25, iou=0.45, device=None):\n",
    "            if device is None:\n",
    "                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            else:\n",
    "                self.device = device\n",
    "                \n",
    "            model_path = f'yolov8{model_size}.pt'\n",
    "            self.model = YOLO(model_path)\n",
    "            \n",
    "            self.conf = conf\n",
    "            self.iou = iou\n",
    "            self.class_names = self.model.names\n",
    "            \n",
    "            print(f\"YOLOv8{model_size} detector initialized on {self.device}\")\n",
    "        \n",
    "        def detect(self, image_path, show_result=True, return_processed_image=False):\n",
    "            result = self.model(image_path, conf=self.conf, iou=self.iou)[0]\n",
    "            \n",
    "            if show_result:\n",
    "                im_array = result.plot()\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Detection Results: {Path(image_path).name}\")\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nDetections in {Path(image_path).name}:\")\n",
    "                boxes = result.boxes\n",
    "                for i, box in enumerate(boxes):\n",
    "                    class_id = int(box.cls.item())\n",
    "                    class_name = self.class_names[class_id]\n",
    "                    confidence = box.conf.item()\n",
    "                    print(f\"  {i+1}. {class_name} (Confidence: {confidence:.2f})\")\n",
    "            \n",
    "            if return_processed_image:\n",
    "                return result.plot()\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        def detect_batch(self, images, show_results=True, max_display=10):\n",
    "            results = self.model(images, conf=self.conf, iou=self.iou)\n",
    "            \n",
    "            if show_results:\n",
    "                display_count = min(len(results), max_display)\n",
    "                rows = (display_count + 2) // 3\n",
    "                cols = min(display_count, 3)\n",
    "                \n",
    "                plt.figure(figsize=(18, 6 * rows))\n",
    "                \n",
    "                for i in range(display_count):\n",
    "                    plt.subplot(rows, cols, i + 1)\n",
    "                    im_array = results[i].plot()\n",
    "                    plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "                    \n",
    "                    if isinstance(images[i], str):\n",
    "                        plt.title(f\"Detection: {Path(images[i]).name}\")\n",
    "                    else:\n",
    "                        plt.title(f\"Detection #{i+1}\")\n",
    "                    \n",
    "                    plt.axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        def print_model_summary(self):\n",
    "            print(f\"YOLOv8 Model Summary:\")\n",
    "            print(f\"Task: {self.model.task}\")\n",
    "            print(f\"Number of classes: {len(self.class_names)}\")\n",
    "            print(f\"Inference device: {self.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the YOLOv8 detector\n",
    "detector = YOLOv8Detector(model_size='n', conf=0.25, iou=0.45)\n",
    "\n",
    "# Print model summary\n",
    "detector.print_model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Uploading and Preparing Images\n",
    "\n",
    "Let's create a function to upload images for object detection. We'll provide options for both uploading your own images and using sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_images_for_detection():\n",
    "    \"\"\"Get images for object detection through upload or samples.\"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Create radio buttons for image source selection\n",
    "    image_source = widgets.RadioButtons(\n",
    "        options=['Upload my own images', 'Use sample images'],\n",
    "        description='Image source:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    display(image_source)\n",
    "    \n",
    "    # Wait for selection\n",
    "    if image_source.value == 'Upload my own images':\n",
    "        # Show upload widget\n",
    "        uploaded = show_upload_widget()\n",
    "        \n",
    "        # Process uploaded files\n",
    "        if 'google.colab' in sys.modules:\n",
    "            image_paths = process_upload_colab(uploaded)\n",
    "        else:\n",
    "            # Wait for user to upload files\n",
    "            print(\"\\nAfter uploading, run the next cell to process the images.\")\n",
    "            return uploaded  # Return the widget for processing in the next cell\n",
    "    else:\n",
    "        # Download and use sample images\n",
    "        image_paths = download_sample_images()\n",
    "        print(f\"\\nUsing {len(image_paths)} sample images.\")\n",
    "    \n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get images for detection\n",
    "image_source = get_images_for_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process uploaded images if needed\n",
    "if not isinstance(image_source, list):\n",
    "    # This means we got a file upload widget\n",
    "    image_paths = process_upload_widget_kaggle(image_source)\n",
    "    print(f\"Processed {len(image_paths)} uploaded images.\")\n",
    "else:\n",
    "    # We already have image paths\n",
    "    image_paths = image_source\n",
    "\n",
    "# Display the images\n",
    "if image_paths:\n",
    "    for path in image_paths:\n",
    "        display_image_with_info(path)\n",
    "else:\n",
    "    print(\"No images available for processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performing Object Detection\n",
    "\n",
    "Now that we have our images, let's perform object detection on them using the YOLOv8 detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect objects in each image\n",
    "if image_paths:\n",
    "    for path in image_paths:\n",
    "        print(f\"\\nProcessing {path}...\")\n",
    "        result = detector.detect(path, show_result=True)\n",
    "else:\n",
    "    print(\"No images available for detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Displaying and Analyzing Results\n",
    "\n",
    "Let's create a function to display more detailed analysis of the detection results, including class distributions and detection confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_detection_results(image_path, result):\n",
    "    \"\"\"Analyze and display detailed detection results.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Get detection boxes\n",
    "    boxes = result.boxes\n",
    "    \n",
    "    if len(boxes) == 0:\n",
    "        print(f\"No objects detected in {Path(image_path).name}\")\n",
    "        return\n",
    "    \n",
    "    # Get class IDs and confidences\n",
    "    class_ids = [int(box.cls.item()) for box in boxes]\n",
    "    confidences = [box.conf.item() for box in boxes]\n",
    "    class_names = [detector.class_names[class_id] for class_id in class_ids]\n",
    "    \n",
    "    # Count objects by class\n",
    "    class_counts = Counter(class_names)\n",
    "    \n",
    "    # Create figure for analysis\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    \n",
    "    # Plot the detection image\n",
    "    plt.subplot(2, 2, 1)\n",
    "    im_array = result.plot()\n",
    "    plt.imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Detection Results: {Path(image_path).name}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    classes, counts = zip(*class_counts.items()) if class_counts else ([], [])\n",
    "    y_pos = np.arange(len(classes))\n",
    "    plt.barh(y_pos, counts, align='center')\n",
    "    plt.yticks(y_pos, classes)\n",
    "    plt.xlabel('Count')\n",
    "    plt.title('Object Classes')\n",
    "    \n",
    "    # Plot confidence distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(confidences, bins=10, range=(0, 1))\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Confidence Distribution')\n",
    "    \n",
    "    # Plot box sizes\n",
    "    if len(boxes) > 0:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        box_areas = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].tolist()\n",
    "            box_areas.append((x2 - x1) * (y2 - y1))\n",
    "        \n",
    "        plt.scatter(range(len(box_areas)), box_areas)\n",
    "        plt.xlabel('Box Index')\n",
    "        plt.ylabel('Box Area (pixelsÂ²)')\n",
    "        plt.title('Bounding Box Sizes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection summary\n",
    "    print(f\"\\nDetection Summary for {Path(image_path).name}:\")\n",
    "    print(f\"Total objects detected: {len(boxes)}\")\n",
    "    print(\"\\nObjects by class:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count}\")\n",
    "    \n",
    "    print(f\"\\nConfidence range: {min(confidences):.2f} - {max(confidences):.2f}\")\n",
    "    print(f\"Average confidence: {sum(confidences)/len(confidences):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze detection results\n",
    "if image_paths:\n",
    "    for path in image_paths:\n",
    "        # Run detection again to get results\n",
    "        result = detector.detect(path, show_result=False)\n",
    "        \n",
    "        # Analyze the results\n",
    "        analyze_detection_results(path, result)\n",
    "else:\n",
    "    print(\"No images available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Processing Multiple Images\n",
    "\n",
    "For efficiency, we can process multiple images in a batch. This is useful when you have many images to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Batch process all images\n",
    "if len(image_paths) > 1:\n",
    "    print(f\"Batch processing {len(image_paths)} images...\")\n",
    "    batch_results = detector.detect_batch(image_paths, show_results=True)\n",
    "    print(f\"Batch processing complete.\")\n",
    "else:\n",
    "    print(\"Need multiple images for batch processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Options for Object Detection\n",
    "\n",
    "Let's create an interactive interface to adjust detection parameters and see how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def interactive_detection(image_path):\n",
    "    \"\"\"Create an interactive interface for object detection.\"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create widgets for parameters\n",
    "    model_size = widgets.Dropdown(\n",
    "        options=['n', 's', 'm', 'l', 'x'],\n",
    "        value='n',\n",
    "        description='Model Size:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    conf_threshold = widgets.FloatSlider(\n",
    "        value=0.25,\n",
    "        min=0.01,\n",
    "        max=0.99,\n",
    "        step=0.05,\n",
    "        description='Confidence:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='.2f',\n",
    "    )\n",
    "    \n",
    "    iou_threshold = widgets.FloatSlider(\n",
    "        value=0.45,\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.05,\n",
    "        description='IoU:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='.2f',\n",
    "    )\n",
    "    \n",
    "    # Create a button to run detection\n",
    "    run_button = widgets.Button(\n",
    "        description='Run Detection',\n",
    "        disabled=False,\n",
    "        button_style='success',\n",
    "        tooltip='Click to run detection with selected parameters',\n",
    "        icon='check'\n",
    "    )\n",
    "    \n",
    "    # Output widget to display results\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Function to run when button is clicked\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            print(f\"Running detection with:\")\n",
    "            print(f\"Model: YOLOv8{model_size.value}\")\n",
    "            print(f\"Confidence threshold: {conf_threshold.value}\")\n",
    "            print(f\"IoU threshold: {iou_threshold.value}\")\n",
    "            \n",
    "            # Initialize detector with selected parameters\n",
    "            detector = YOLOv8Detector(\n",
    "                model_size=model_size.value,\n",
    "                conf=conf_threshold.value,\n",
    "                iou=iou_threshold.value\n",
    "            )\n",
    "            \n",
    "            # Run detection\n",
    "            result = detector.detect(image_path, show_result=True)\n",
    "            \n",
    "            # Show detailed analysis\n",
    "            analyze_detection_results(image_path, result)\n",
    "    \n",
    "    # Connect the button to the function\n",
    "    run_button.on_click(on_button_clicked)\n",
    "    \n",
    "    # Display the widgets\n",
    "    print(f\"Interactive Detection for {Path(image_path).name}\")\n",
    "    print(\"Select parameters and click 'Run Detection' to see results.\")\n",
    "    display(widgets.VBox([\n",
    "        widgets.HBox([model_size, conf_threshold, iou_threshold]),\n",
    "        run_button\n",
    "    ]))\n",
    "    display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run interactive detection on the first image\n",
    "if image_paths:\n",
    "    interactive_detection(image_paths[0])\n",
    "else:\n",
    "    print(\"No images available for interactive detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving Detection Results\n",
    "\n",
    "Let's create a function to save the detection results to disk, which can be useful for further analysis or sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_detection_results(image_paths, output_dir='detection_results'):\n",
    "    \"\"\"Save detection results to disk.\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import cv2\n",
    "    import json\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process each image\n",
    "    results = []\n",
    "    for i, path in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}: {Path(path).name}\")\n",
    "        \n",
    "        # Run detection\n",
    "        result = detector.detect(path, show_result=False)\n",
    "        \n",
    "        # Get detection image\n",
    "        detection_img = result.plot()\n",
    "        \n",
    "        # Save detection image\n",
    "        output_path = os.path.join(output_dir, f\"detection_{Path(path).stem}.jpg\")\n",
    "        cv2.imwrite(output_path, detection_img)\n",
    "        \n",
    "        # Prepare detection data for JSON\n",
    "        boxes = result.boxes\n",
    "        detection_data = {\n",
    "            'image_name': Path(path).name,\n",
    "            'detections': []\n",
    "        }\n",
    "        \n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls.item())\n",
    "            class_name = detector.class_names[class_id]\n",
    "            confidence = float(box.conf.item())\n",
    "            bbox = [float(x) for x in box.xyxy[0].tolist()]  # xyxy format is [x1, y1, x2, y2]\n",
    "            \n",
    "            detection_data['detections'].append({\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_name,\n",
    "                'confidence': confidence,\n",
    "                'bbox': bbox\n",
    "            })\n",
    "        \n",
    "        # Save detection data to JSON\n",
    "        json_path = os.path.join(output_dir, f\"detection_{Path(path).stem}.json\")\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(detection_data, f, indent=2)\n",
    "        \n",
    "        results.append({\n",
    "            'image_path': path,\n",
    "            'detection_image': output_path,\n",
    "            'detection_data': json_path,\n",
    "            'num_detections': len(boxes)\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nSaved detection results for {len(image_paths)} images to {output_dir}/\")\n",
    "    \n",
    "    # If in Colab, provide a download link\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import files\n",
    "        \n",
    "        # Zip the results directory\n",
    "        !zip -r {output_dir}.zip {output_dir}\n",
    "        \n",
    "        # Download the zip file\n",
    "        print(\"\\nDownloading results as a zip file...\")\n",
    "        files.download(f\"{output_dir}.zip\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save detection results\n",
    "if image_paths:\n",
    "    saved_results = save_detection_results(image_paths)\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\nDetection Results Summary:\")\n",
    "    for result in saved_results:\n",
    "        print(f\"  {Path(result['image_path']).name}: {result['num_detections']} objects detected\")\n",
    "else:\n",
    "    print(\"No images available for saving results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Set up the environment for object detection\n",
    "2. Import and configure the YOLOv8 detector\n",
    "3. Upload and prepare images for detection\n",
    "4. Perform object detection on individual images\n",
    "5. Analyze and visualize detection results\n",
    "6. Batch process multiple images\n",
    "7. Use interactive controls to adjust detection parameters\n",
    "8. Save detection results to disk\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore how to perform real-time object detection using a webcam or video feed. This will allow you to detect objects in a live video stream, which is useful for applications like surveillance, robotics, and interactive systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}